## 随机梯度下降

#### 概念

Stochastic Gradient Descent  随机梯度下降(SGD) 是一种简单但又非常高效的方法，主要用于凸损失函数下线性分类器的判别式学习

- 优势:
    - 高效 
    - 易于实现 (有大量优化代码的机会)
    
- 缺点:
    - SGD 需要一些超参数，例如 regularization （正则化）参数和 number of iterations （迭代次数）
    - SGD 对 feature scaling （特征缩放）敏感
 

