## 卷积神经网络
[卷积介绍](http://www.sohu.com/a/241338315_787107)

#### 卷积
卷积过程是基于一个小矩阵，也就是卷积核，在上面所说的每层像素矩阵上不断按步长扫过去的，扫到数与卷积核对应位置的数相乘，然后求总和，每扫一次，得到一个值，全部扫完则生成一个新的矩阵

- 卷积核的步长是指卷积核每次移动几个格子，有横行和纵向两个方向。
- 卷积操作相当于特征提取，卷积核相当于一个过滤器，提取我们需要的特征。

#### Padding
卷积操作之后维度变少，得到的矩阵比原来矩阵小，这样不好计算，而我们只是希望作卷积，所以我们需要Padding，在每次卷积操作之前，在原矩阵外边补包一层0，可以只在横向补，或只在纵向补，或者四周都补0，从而使得卷积后输出的图像跟输入图像在尺寸上一致。

- Full模式：第一个窗口只包含1个输入的元素，即从卷积核（fileter）和输入刚相交开始做卷积。没有元素的部分做补0操作。
- Valid模式：卷积核和输入完全相交开始做卷积，这种模式不需要补0。
- Same模式：当卷积核的中心C和输入开始相交时做卷积。没有元素的部分做补0操作。

#### 池化(pooling)
卷积后提取了很多特征信息,相邻区域有相似的特征信息,为了减少计算量,需要做降维操作,就是池化

##### 池化方法
- 最大值
- 平均值
- 最小值

#### Flatten

Flatten 是指将多维的矩阵拉开，变成一维向量来表示

#### 全连接层(Dense)
对n-1层和n层而言，n-1层的任意一个节点，都和第n层所有节点有连接。即第n层的每个节点在进行计算的时候，激活函数的输入是n-1层所有节点的加权。像下面的中间层就是全连接方式

#### Dropout

dropout是指在网络的训练过程中，按照一定的概率将网络中的神经元丢弃，这样有效防止过拟合

#### 

