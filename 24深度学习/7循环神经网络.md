## 循环神经网络(RNN)

[LSTM和GRU的解析](https://mp.weixin.qq.com/s/aV9Rj-CnJZRXRm0rDOK6gg)
[动图讲解RNN](https://zhuanlan.zhihu.com/p/36455374)
[讲解LSTM](https://www.cnblogs.com/mfryf/p/7904017.html)

#### 什么是RNN
RNN 适用于处理序列数据用于预测，但却受到短时记忆的制约

#### 为什么这么设计

#### 如何使用

### 延伸

#### LSTM
长短期记忆网络（Long Short Term Memory networks） - 通常叫做 “LSTMs” —— 是 RNN 中一个特殊的类型
[理解LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
```bash
遗忘门:决定让那些信息继续通过这个 cell
传入门:决定让多少新的信息加入到 cell 状态 中来
输出门:决定输出什么值
```
LSTM 输入是三维的
样品 ： 一个序列是一个样本。批次由一个或多个样本组成。
时间步 ： 一个时间步代表样本中的一个观察点。 （50个时间步长，2个特征）
特征 ： 一个特征是在一个时间步长的观察得到的。

##### 模型
![lstm图片](./.image/lstm.jpeg)
- one to one
一个时间步输出一个值
 ```bash
X = seq.reshape(len(seq), 1, 1) # 5 个样本 一个时间步 一个特征
y = seq.reshape(len(seq), 1) # 5个样本 1个特征

length = 5
n_neurons = length
# create LSTM
model = Sequential()
model.add(LSTM(n_neurons, input_shape=(1, 1)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
print(model.summary())

# train LSTM
model.fit(X, y, epochs=n_epoch, batch_size=n_batch, verbose=2)
```

- many to one
模型一次性输出序列 ： 输出一个向量（5个时间步输出一个），而不是一个序列（每个时间步输出一个）
```bash
X = seq.reshape(1, 5, 1) # 一个样本，5个时间步， 一个特征
y = seq.reshape(1, 5) #  一个样本 5个特征

# create LSTM
length = 5
model = Sequential()
model.add(LSTM(5, input_shape=(5, 1)))
model.add(Dense(length))  # 5 个神经元的全连接层
model.compile(loss='mean_squared_error', optimizer='adam')
print(model.summary())
```

- many to many(with TimeDistributed)
使用 TimeDistributed 的 要点：
    1. 输入必须是 3D； 需要配置 TimeDistributed dense 层的前一层 返回序列
    2. 输出可能是 3D ： 如果 TimeDistributed dense 层 是输出层，输出一个序列，可能需要将 y reshape into 3D
    3. 使用 TimeDistributedDense ，计算出来的 损失函数是所有时间步上面的输出，不使用的话，损失只是最后一步的输出
```bash
# 每个 LSTM unit 返回一个 包含五个输出值的序列，每个输出对应输入数据的一个时间步
model.add(LSTM(n_neurons, input_shape=(length, 1), return_sequences=True))
# 使用 TimeDistributed 包裹 单个输出的的 全连接 dense 层
model.add(TimeDistributed(Dense(1)))
```


#### GRU

```bash
重置门:决定是否将之前的状态忘记
更新门:决定是否要将隐藏状态更新为新的状态ht~（作用相当于 LSTM 中的输出门）
```

#### LSTM和GRU的区别

    - GRU 去除掉了细胞状态
    - 使用隐藏状态来进行信息的传递。它只包含两个门：更新门和重置门
    - GRU 的张量运算较少，因此它比 LSTM 的训练更快一下

#### 应用

- 机器翻译