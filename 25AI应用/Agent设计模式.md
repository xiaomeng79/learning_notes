# AI Agent 系统的各种设计模式

## 提示词链
- 任务分解：将复杂任务拆解为聚焦步骤序列（又称管道模式）
- 链式处理：每步使用前步输出作为输入，执行LLM调用或处理逻辑
- 可靠性提升：显著增强复杂语言交互的可控性和稳定性

## 路由
- 路由使 Agent 能够根据条件动态决定工作流中的下一步。
- 它允许 Agent 处理各种输入并调整其行为，超越线性执行。
- 路由逻辑可以使用 LLM、基于规则的系统或嵌入相似性实现。

## 并行化
- 并行化是一种通过并发执行独立任务来提高效率的模式
- 在涉及等待外部资源（如 API 调用）的任务中特别有效
- 采用并发或并行架构会引入显著复杂性和成本，影响设计、调试和系统日志等关键开发环节
- 并行化有助于减少整体延迟，使 Agent 系统在处理复杂任务时更具响应性
- 像 LangChain 和 Google ADK 这样的框架提供定义和管理并行执行的内置支持

## 反思
- 反思模式的主要优势在于其迭代自我纠正和优化输出的能力，带来显著更高的质量、准确性和复杂指令遵循度
- 它涉及执行、评估/评审和优化的反馈循环。反思对需要高质量、准确或精细输出的任务至关重要
- 一个强大的实现是生产者-评审者模型，其中独立 Agent（或提示角色）评估初始输出。这种关注点分离增强客观性，并支持更专业、结构化的反馈
- 然而，这些优势以增加的延迟和计算成本为代价，同时伴随超出模型上下文窗口或被 API 服务限制的更高风险
- 此模式使 Agent 能够执行自我纠正并随时间提升性能

## 工具使用
- 工具使用（函数调用）允许 Agent 与外部系统交互并访问动态信息。
- 它涉及定义具有 LLM 可以理解的清晰描述和参数的工具。
- LLM 决定何时使用工具并生成结构化函数调用。
- Agent 框架执行实际的工具调用并将结果返回给 LLM。
- 工具使用对于构建可以执行现实世界操作并提供最新信息的 Agent 至关重要。

## 规划
- 规划使 Agent 能够将复杂目标分解为可操作的顺序步骤。
- 它对于处理多步任务、工作流自动化和导航复杂环境至关重要。
- LLM 可以通过基于任务描述生成逐步方法来执行规划。
- 明确提示或设计任务以要求规划步骤会在 Agent 框架中鼓励这种行为。

## 多 Agent 协作
- 多 Agent 协作涉及多个 Agent 协同工作以实现共同目标。
- 此模式利用专业角色、分布式任务和 Agent 间通信。
- 协作可以采取顺序交接、并行处理、辩论或层次结构等形式。
- 此模式非常适合需要多样化专业知识或多个不同阶段的复杂问题。

## 记忆管理
- 短期记忆（上下文记忆）：
- 长期记忆（持久记忆）：数据库、知识图谱或向量数据库中
  - 长期记忆类型：
    - 语义记忆：记住事实 涉及保留特定事实和概念，如用户偏好或领域知识。
    - 情景记忆：记住经历 涉及回忆过去事件或行动。
    - 程序记忆：记住规则 关于如何执行任务的记忆——Agent 的核心指令和行为，通常包含在其系统提示中。
- 每次与 Agent 交互可视为独特对话线程，Agent 可能需要访问早期数据。
  - Session（会话）： 独立聊天线程，记录该特定交互的消息和操作（Events），同时存储与该对话相关的临时数据（State）
  - State（状态）（session.state）： 存储在 Session 中的数据，包含仅与当前活动聊天线程相关的信息
  - Memory（记忆）： 来自各种过往聊天或外部来源信息的可搜索存储库，作为超出即时对话范围的数据检索资源

## 学习和适应
- 关键学习机制：
    - 强化学习：通过奖励（积极结果）和惩罚（消极结果）在交互中学习最优行为，例如训练游戏角色或机器人。
    - 监督学习与无监督学习：分别从带标签示例中学习输入-输出映射，或在未标注数据中发现隐藏模式。
    - 基于记忆的学习：回忆过去经验以调整当前行动，增强决策能力。
    - 在线学习：持续用新数据更新知识，适应动态环境
- 重要算法：
  - 近端策略优化（PPO）：通过小幅、谨慎的策略更新（使用“裁剪”机制避免剧烈变化），稳定地改进智能体在连续动作环境中的决策。
  - 直接偏好优化（DPO）：简化大语言模型与人类偏好的对齐过程，直接利用偏好数据更新模型，无需训练复杂的奖励模型。
- 经验法则：
  - 在动态、不确定或需个性化的环境中构建智能体时，集成学习和适应机制至关重要。

## 

