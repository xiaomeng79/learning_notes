## 梯度下降

#### 优化
- 参数归一化(不管是从那个方向,更容易找到最小值)


#### 梯度消失和梯度爆炸
- 随机化参数



#### 动量梯度下降法Momentum
- 作用: 动量梯度下降法不但能使用较大的学习率，其迭代次数也较少
- 理论: 指数加权平均
- 其他: 与Momentum的机制类似的，还有一种叫做RMSprop的算法，他们俩的效果也是类似，都解决了相同的问题，使梯度下降时的折返情况减轻，从而加快训练速度。因为下降的路线更接近同一个方向，因此也可以将学习率增大来加快训练速度
- 拓展: Momentum和RMSprop的结合，产生了一种权威算法叫做Adam，Adam结合了前两者的计算方式形成自己的优化方法，基本适用于所有的模型和网络结构
